{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit these to point at where the Level 5 dataset lives on your machine.\n",
    "DATASET_VERSION = 'v1.02-train'\n",
    "DATASET_ROOT = '../level5dataset/v1.02-train/'\n",
    "\n",
    "# Our code will generate data, visualization and model checkpoints, they will be persisted to disk in this folder\n",
    "ARTIFACTS_FOLDER = \"artifacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Disable multiprocesing for numpy/opencv. We already multiprocess ourselves, this would mean every subprocess produces\n",
    "# even more threads which would lead to a lot of context switching, slowing things down a lot.\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import scipy.special\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud, Box, Quaternion\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "18421 instance,\n",
      "10 sensor,\n",
      "148 calibrated_sensor,\n",
      "177789 ego_pose,\n",
      "180 log,\n",
      "180 scene,\n",
      "22680 sample,\n",
      "189504 sample_data,\n",
      "638179 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 17.6 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 4.6 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "level5data = LyftDataset(data_path='.', json_path='train_data', verbose=True)\n",
    "os.makedirs(ARTIFACTS_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_folder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-79a1747748b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0minput_filepaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"*_input.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mtarget_filepaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"*_target.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mmap_filepaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"*_map.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data_folder' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "class BEVImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_filepaths, target_filepaths, map_filepaths=None):\n",
    "        self.input_filepaths = input_filepaths\n",
    "        self.target_filepaths = target_filepaths\n",
    "        self.map_filepaths = map_filepaths\n",
    "        \n",
    "        if map_filepaths is not None:\n",
    "            assert len(input_filepaths) == len(map_filepaths)\n",
    "        \n",
    "        assert len(input_filepaths) == len(target_filepaths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_filepath = self.input_filepaths[idx]\n",
    "        target_filepath = self.target_filepaths[idx]\n",
    "        \n",
    "        sample_token = input_filepath.split(\"/\")[-1].replace(\"_input.png\",\"\")\n",
    "        \n",
    "        im = cv2.imread(input_filepath, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        if self.map_filepaths:\n",
    "            map_filepath = self.map_filepaths[idx]\n",
    "            map_im = cv2.imread(map_filepath, cv2.IMREAD_UNCHANGED)\n",
    "            im = np.concatenate((im, map_im), axis=2)\n",
    "        \n",
    "        target = cv2.imread(target_filepath, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        im = im.astype(np.float32)/255\n",
    "        target = target.astype(np.int64)\n",
    "        \n",
    "        im = torch.from_numpy(im.transpose(2,0,1))\n",
    "        target = torch.from_numpy(target)\n",
    "        \n",
    "        return im, target, sample_token\n",
    "\n",
    "input_filepaths = sorted(glob.glob(os.path.join(train_data_folder, \"*_input.png\")))\n",
    "target_filepaths = sorted(glob.glob(os.path.join(train_data_folder, \"*_target.png\")))\n",
    "map_filepaths = sorted(glob.glob(os.path.join(train_data_folder, \"*_map.png\")))\n",
    "\n",
    "train_dataset = BEVImageDataset(input_filepaths, target_filepaths, map_filepaths)\n",
    "    \n",
    "im, target, sample_token = train_dataset[1]\n",
    "im = im.numpy()\n",
    "target = target.numpy()\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "target_as_rgb = np.repeat(target[...,None], 3, 2)\n",
    "# Transpose the input volume CXY to XYC order, which is what matplotlib requires.\n",
    "plt.imshow(np.hstack((im.transpose(1,2,0)[...,:3], im.transpose(1,2,0)[...,3:], target_as_rgb)))\n",
    "plt.title(sample_token)\n",
    "plt.show()\n",
    "\n",
    "visualize_lidar_of_sample(sample_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [(level5data.get('sample', record['first_sample_token'])['timestamp'], record) for record in\n",
    "        level5data.scene]\n",
    "\n",
    "entries = []\n",
    "\n",
    "for start_time, record in sorted(records):\n",
    "    start_time = level5data.get('sample', record['first_sample_token'])['timestamp'] / 1000000\n",
    "\n",
    "    token = record['token']\n",
    "    name = record['name']\n",
    "    date = datetime.utcfromtimestamp(start_time)\n",
    "    host = \"-\".join(record['name'].split(\"-\")[:2])\n",
    "    first_sample_token = record[\"first_sample_token\"]\n",
    "\n",
    "    entries.append((host, name, date, token, first_sample_token))\n",
    "            \n",
    "df = pd.DataFrame(entries, columns=[\"host\", \"scene_name\", \"date\", \"scene_token\", \"first_sample_token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_count_df = df.groupby(\"host\")['scene_token'].count()\n",
    "print(host_count_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_hosts = [\"host-a007\", \"host-a008\", \"host-a009\"]\n",
    "\n",
    "validation_df = df[df[\"host\"].isin(validation_hosts)]\n",
    "vi = validation_df.index\n",
    "train_df = df[~df.index.isin(vi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"car\", \"motorcycle\", \"bus\", \"bicycle\", \"truck\", \"pedestrian\", \"other_vehicle\", \"animal\", \"emergency_vehicle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_token = train_df.first_sample_token.values[0]\n",
    "sample = level5data.get(\"sample\", sample_token)\n",
    "\n",
    "sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)\n",
    "\n",
    "ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "calibrated_sensor = level5data.get(\"calibrated_sensor\", lidar_data[\"calibrated_sensor_token\"])\n",
    "\n",
    "# Homogeneous transformation matrix from car frame to world frame.\n",
    "global_from_car = transform_matrix(ego_pose['translation'],\n",
    "                                   Quaternion(ego_pose['rotation']), inverse=False)\n",
    "\n",
    "# Homogeneous transformation matrix from sensor coordinate frame to ego car frame.\n",
    "car_from_sensor = transform_matrix(calibrated_sensor['translation'], Quaternion(calibrated_sensor['rotation']),\n",
    "                                    inverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_pointcloud = LidarPointCloud.from_file(lidar_filepath)\n",
    "\n",
    "# The lidar pointcloud is defined in the sensor's reference frame.\n",
    "# We want it in the car's reference frame, so we transform each point\n",
    "lidar_pointcloud.transform(car_from_sensor)\n",
    "\n",
    "# A sanity check, the points should be centered around 0 in car space.\n",
    "plt.hist(lidar_pointcloud.points[0], alpha=0.5, bins=30, label=\"X\")\n",
    "plt.hist(lidar_pointcloud.points[1], alpha=0.5, bins=30, label=\"Y\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Distance from car along axis\")\n",
    "plt.ylabel(\"Amount of points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_mask = level5data.map[0][\"mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_map_around_ego(map_mask, ego_pose, voxel_size, output_shape):\n",
    "\n",
    "    def crop_image(image: np.array,\n",
    "                           x_px: int,\n",
    "                           y_px: int,\n",
    "                           axes_limit_px: int) -> np.array:\n",
    "                x_min = int(x_px - axes_limit_px)\n",
    "                x_max = int(x_px + axes_limit_px)\n",
    "                y_min = int(y_px - axes_limit_px)\n",
    "                y_max = int(y_px + axes_limit_px)\n",
    "\n",
    "                cropped_image = image[y_min:y_max, x_min:x_max]\n",
    "\n",
    "                return cropped_image\n",
    "\n",
    "    pixel_coords = map_mask.to_pixel_coords(ego_pose['translation'][0], ego_pose['translation'][1])\n",
    "\n",
    "    extent = voxel_size*output_shape[0]*0.5\n",
    "    scaled_limit_px = int(extent * (1.0 / (map_mask.resolution)))\n",
    "    mask_raster = map_mask.mask()\n",
    "\n",
    "    cropped = crop_image(mask_raster, pixel_coords[0], pixel_coords[1], int(scaled_limit_px * np.sqrt(2)))\n",
    "\n",
    "    ypr_rad = Quaternion(ego_pose['rotation']).yaw_pitch_roll\n",
    "    yaw_deg = -np.degrees(ypr_rad[0])\n",
    "\n",
    "    rotated_cropped = np.array(Image.fromarray(cropped).rotate(yaw_deg))\n",
    "    ego_centric_map = crop_image(rotated_cropped, rotated_cropped.shape[1] / 2, rotated_cropped.shape[0] / 2,\n",
    "                                 scaled_limit_px)[::-1]\n",
    "    \n",
    "    ego_centric_map = cv2.resize(ego_centric_map, output_shape[:2], cv2.INTER_NEAREST)\n",
    "    return ego_centric_map.astype(np.float32)/255\n",
    "    \n",
    "ego_centric_map = get_semantic_map_around_ego(map_mask, ego_pose, voxel_size=0.4, output_shape=(336,336)) \n",
    "plt.imshow(ego_centric_map)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
